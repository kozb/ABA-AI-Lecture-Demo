{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Lecture 4: Advanced Prompt Engineering\n",
        "\n",
        "## Working with TinyLlama via Ollama\n",
        "\n",
        "**Learning Objectives:**\n",
        "- Understand how inference parameters (temperature) affect model output\n",
        "- Learn Chain-of-Thought (CoT) prompting techniques\n",
        "- Explore system prompts and persona-based prompting\n",
        "- Experience the limitations of small models (<1GB) and why effective prompting is critical\n",
        "\n",
        "**Note:** We're using TinyLlama (~637MB), a very small model. This demonstrates that even with limited capabilities, proper prompting techniques can significantly improve results.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 1. Setup & Initialization\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install Ollama\n",
        "!curl -fsSL https://ollama.com/install.sh | sh\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install Python wrapper for Ollama\n",
        "!pip install ollama\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Start Ollama server in the background\n",
        "# CRITICAL: Using subprocess.Popen to run ollama serve as a background process\n",
        "# This prevents Colab from blocking while the server runs\n",
        "import subprocess\n",
        "import time\n",
        "\n",
        "# Start ollama serve in the background\n",
        "ollama_process = subprocess.Popen(\n",
        "    ['ollama', 'serve'],\n",
        "    stdout=subprocess.PIPE,\n",
        "    stderr=subprocess.PIPE\n",
        ")\n",
        "\n",
        "# Wait for the server to start (5 seconds)\n",
        "print(\"â³ Starting Ollama server...\")\n",
        "time.sleep(5)\n",
        "print(\"âœ… Ollama server should be running in the background\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Pull the TinyLlama model\n",
        "!ollama pull tinyllama\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 2. Helper Function\n",
        "\n",
        "Create a reusable function to query the model with customizable parameters.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import ollama\n",
        "\n",
        "def query_model(prompt, temperature=0.7, system_prompt=\"\"):\n",
        "    \"\"\"\n",
        "    Query TinyLlama model via Ollama.\n",
        "    \n",
        "    Args:\n",
        "        prompt (str): The user prompt/question\n",
        "        temperature (float): Controls randomness (0.0 = deterministic, 1.0+ = creative)\n",
        "        system_prompt (str): Optional system prompt to set model behavior/persona\n",
        "    \n",
        "    Returns:\n",
        "        str: Model's response\n",
        "    \"\"\"\n",
        "    # Prepare the request\n",
        "    messages = []\n",
        "    \n",
        "    # Add system prompt if provided\n",
        "    if system_prompt:\n",
        "        messages.append({\n",
        "            \"role\": \"system\",\n",
        "            \"content\": system_prompt\n",
        "        })\n",
        "    \n",
        "    # Add user prompt\n",
        "    messages.append({\n",
        "        \"role\": \"user\",\n",
        "        \"content\": prompt\n",
        "    })\n",
        "    \n",
        "    # Query the model\n",
        "    response = ollama.chat(\n",
        "        model='tinyllama',\n",
        "        messages=messages,\n",
        "        options={\n",
        "            'temperature': temperature\n",
        "        }\n",
        "    )\n",
        "    \n",
        "    return response['message']['content']\n",
        "\n",
        "print(\"âœ… Helper function 'query_model' created\")\n",
        "print(\"   Usage: query_model(prompt, temperature=0.7, system_prompt='')\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 3. Demo 1: Inference Parameters - Temperature\n",
        "\n",
        "### Understanding Temperature\n",
        "\n",
        "**Temperature** controls the randomness/creativity of model outputs:\n",
        "- **Low Temperature (0.1-0.3)**: More deterministic, focused, consistent\n",
        "- **High Temperature (0.7-1.5)**: More creative, diverse, unpredictable\n",
        "\n",
        "**Task:** Generate a 4-line poem about a lonely robot on Mars with different temperature settings.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Demo 1: Temperature Effects\n",
        "prompt = \"Write a 4-line poem about a lonely robot on Mars.\"\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"Low Temp (0.1) - Deterministic\")\n",
        "print(\"=\" * 70)\n",
        "output_low = query_model(prompt, temperature=0.1)\n",
        "print(output_low)\n",
        "print()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\" * 70)\n",
        "print(\"High Temp (1.0) - Creative\")\n",
        "print(\"=\" * 70)\n",
        "output_high = query_model(prompt, temperature=1.0)\n",
        "print(output_high)\n",
        "print()\n",
        "\n",
        "print(\"ðŸ’¡ Notice the difference in creativity and variation between the two outputs!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 4. Demo 2: Chain-of-Thought (CoT) Prompting\n",
        "\n",
        "### What is Chain-of-Thought?\n",
        "\n",
        "**Chain-of-Thought (CoT)** prompting encourages the model to \"think step by step\" before providing an answer. This is especially important for reasoning tasks like math problems.\n",
        "\n",
        "**Why it matters for small models:** TinyLlama has limited reasoning capabilities. Breaking down complex problems into steps helps it process information more effectively.\n",
        "\n",
        "**Task:** Solve `123 * 76` with and without CoT prompting.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Demo 2: Chain-of-Thought - Baseline (No CoT)\n",
        "prompt_baseline = \"What is 123 * 76? Answer with just the number.\"\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"Baseline (No CoT)\")\n",
        "print(\"=\" * 70)\n",
        "print(f\"Prompt: {prompt_baseline}\")\n",
        "print()\n",
        "result_baseline = query_model(prompt_baseline, temperature=0.1)\n",
        "print(f\"Model Output: {result_baseline}\")\n",
        "print()\n",
        "print(f\"Expected Answer: 9348\")\n",
        "print()\n",
        "print(\"âš ï¸ Note: TinyLlama may struggle with this calculation due to its small size.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Demo 2: Chain-of-Thought - With Step-by-Step Reasoning\n",
        "prompt_cot = \"\"\"What is 123 * 76? Let's think step by step. \n",
        "Multiply 100*76, then 20*76, then 3*76, and add them up.\"\"\"\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"Chain-of-Thought (CoT)\")\n",
        "print(\"=\" * 70)\n",
        "print(f\"Prompt: {prompt_cot}\")\n",
        "print()\n",
        "result_cot = query_model(prompt_cot, temperature=0.1)\n",
        "print(f\"Model Output: {result_cot}\")\n",
        "print()\n",
        "print(f\"Expected Answer: 9348\")\n",
        "print()\n",
        "print(\"ðŸ’¡ Compare: Does the step-by-step approach help TinyLlama get closer to the correct answer?\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 5. Assignment: The \"Persona\" Challenge\n",
        "\n",
        "### System Prompts and Role-Playing\n",
        "\n",
        "**System prompts** allow you to set the model's behavior, tone, and expertise level. This is powerful for:\n",
        "- Adapting explanations to different audiences\n",
        "- Role-playing scenarios\n",
        "- Controlling output style\n",
        "\n",
        "**Your Task:** Test TinyLlama's ability to adapt its explanations based on different personas.\n",
        "\n",
        "**Note:** Since TinyLlama is a small model (<1GB), it may not perfectly follow complex personas, but you'll see how system prompts influence its responses.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Assignment: Persona Challenge\n",
        "# Task: Ask TinyLlama to explain Quantum Entanglement with different personas\n",
        "\n",
        "prompt = \"Explain Quantum Entanglement\"\n",
        "\n",
        "# Challenge A: 5-year-old teacher persona\n",
        "# TODO: Fill in the system_prompt below\n",
        "system_prompt_a = \"\"  # You are a specialized 5-year-old teacher. Speak in simple words.\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"Challenge A: 5-Year-Old Teacher Persona\")\n",
        "print(\"=\" * 70)\n",
        "print(f\"System Prompt: {system_prompt_a if system_prompt_a else '(Not filled in yet)'}\")\n",
        "print()\n",
        "if system_prompt_a:\n",
        "    result_a = query_model(prompt, temperature=0.7, system_prompt=system_prompt_a)\n",
        "    print(result_a)\n",
        "else:\n",
        "    print(\"âš ï¸ Please fill in system_prompt_a above and re-run this cell\")\n",
        "print()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Challenge B: Nobel Prize Physicist persona\n",
        "# TODO: Fill in the system_prompt below\n",
        "system_prompt_b = \"\"  # You are a Nobel Prize Physicist. Use technical jargon.\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"Challenge B: Nobel Prize Physicist Persona\")\n",
        "print(\"=\" * 70)\n",
        "print(f\"System Prompt: {system_prompt_b if system_prompt_b else '(Not filled in yet)'}\")\n",
        "print()\n",
        "if system_prompt_b:\n",
        "    result_b = query_model(prompt, temperature=0.7, system_prompt=system_prompt_b)\n",
        "    print(result_b)\n",
        "else:\n",
        "    print(\"âš ï¸ Please fill in system_prompt_b above and re-run this cell\")\n",
        "print()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Assignment Instructions\n",
        "\n",
        "1. **Fill in the system prompts** in the cells above:\n",
        "   - Challenge A: `\"You are a specialized 5-year-old teacher. Speak in simple words.\"`\n",
        "   - Challenge B: `\"You are a Nobel Prize Physicist. Use technical jargon.\"`\n",
        "\n",
        "2. **Run both cells** and observe the differences in:\n",
        "   - Vocabulary complexity\n",
        "   - Explanation depth\n",
        "   - Tone and style\n",
        "\n",
        "3. **Reflection Questions:**\n",
        "   - How well does TinyLlama adapt to different personas?\n",
        "   - What limitations do you notice with a small model?\n",
        "   - How might system prompts be useful in real applications?\n",
        "\n",
        "**Expected Observations:**\n",
        "- TinyLlama may struggle to perfectly match personas due to its size\n",
        "- However, you should still see some differences in vocabulary and style\n",
        "- This demonstrates why effective prompting is critical, especially for smaller models\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Key Takeaways\n",
        "\n",
        "**What we learned:**\n",
        "- âœ… How temperature affects model creativity and determinism\n",
        "- âœ… Chain-of-Thought prompting for complex reasoning tasks\n",
        "- âœ… System prompts for persona-based and role-playing scenarios\n",
        "- âœ… The importance of effective prompting, especially for smaller models\n",
        "\n",
        "**Why TinyLlama?**\n",
        "- Demonstrates that even small models (<1GB) can benefit from proper prompting\n",
        "- Shows the limitations of small models, making effective techniques even more critical\n",
        "- Fast inference allows for rapid experimentation and learning\n",
        "\n",
        "**Next Steps:**\n",
        "- Experiment with different temperature values\n",
        "- Try CoT prompting on other reasoning tasks\n",
        "- Explore more complex system prompts and multi-turn conversations\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}

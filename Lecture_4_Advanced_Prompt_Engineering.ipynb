{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Lecture 4: Advanced Prompt Engineering\n",
        "\n",
        "## üéØ Master the Art of Communicating with AI Models\n",
        "\n",
        "Welcome to **Advanced Prompt Engineering**! In this lecture, you'll learn how to unlock the full potential of language models through effective prompting techniques. Just like learning a new language, mastering prompt engineering is about understanding how to communicate clearly and effectively with AI systems.\n",
        "\n",
        "---\n",
        "\n",
        "## üìö Learning Objectives\n",
        "\n",
        "By the end of this lecture, you will be able to:\n",
        "\n",
        "- **Control model behavior** using inference parameters (temperature)\n",
        "- **Improve reasoning accuracy** with Chain-of-Thought (CoT) prompting\n",
        "- **Customize model responses** using system prompts and personas\n",
        "- **Apply best practices** for effective prompt engineering in real-world scenarios\n",
        "\n",
        "---\n",
        "\n",
        "## üõ†Ô∏è Technical Setup\n",
        "\n",
        "**Model:** Llama 3.2 3B (~2GB) via Ollama  \n",
        "**Why this model?** A capable small model that demonstrates how proper prompting techniques can dramatically improve results, especially for reasoning tasks like math calculations. Perfect for learning and experimentation!\n",
        "\n",
        "---\n",
        "\n",
        "## üí° Why Prompt Engineering Matters\n",
        "\n",
        "Prompt engineering is the difference between getting mediocre results and unlocking exceptional AI performance. The same model can produce vastly different outputs based on how you ask the question. In this demo, you'll see firsthand how small changes in prompts can lead to significant improvements in accuracy, relevance, and usefulness.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 1. üöÄ Setup & Initialization\n",
        "\n",
        "Let's get everything set up to start working with our language model. We'll use **Ollama**, a powerful tool that allows us to run large language models locally.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 1: Install Ollama\n",
        "# Ollama allows us to run LLMs locally without needing cloud APIs\n",
        "!curl -fsSL https://ollama.com/install.sh | sh\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 2: Install Python wrapper for Ollama\n",
        "# This gives us a Python interface to interact with Ollama\n",
        "!pip install ollama\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 3: Start Ollama server in the background\n",
        "# CRITICAL: Using subprocess.Popen to run ollama serve as a background process\n",
        "# This prevents the notebook from blocking while the server runs\n",
        "import subprocess\n",
        "import time\n",
        "\n",
        "# Start ollama serve in the background\n",
        "ollama_process = subprocess.Popen(\n",
        "    ['ollama', 'serve'],\n",
        "    stdout=subprocess.PIPE,\n",
        "    stderr=subprocess.PIPE\n",
        ")\n",
        "\n",
        "# Wait for the server to start\n",
        "print(\"‚è≥ Starting Ollama server...\")\n",
        "time.sleep(5)\n",
        "print(\"‚úÖ Ollama server is running in the background\")\n",
        "print(\"   Ready to load models!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 4: Download the Llama 3.2 3B model\n",
        "# This will download ~2GB - may take a few minutes depending on your connection\n",
        "print(\"üì• Downloading Llama 3.2 3B model...\")\n",
        "print(\"   This is a one-time download. The model will be cached locally.\")\n",
        "!ollama pull llama3.2:3b\n",
        "print(\"‚úÖ Model downloaded and ready to use!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 2. üîß Helper Function\n",
        "\n",
        "Let's create a reusable function to interact with our model. This will make our code cleaner and easier to experiment with different prompts and parameters.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import ollama\n",
        "\n",
        "def query_model(prompt, temperature=0.7, system_prompt=\"\"):\n",
        "    \"\"\"\n",
        "    Query Llama 3.2 3B model via Ollama with customizable parameters.\n",
        "    \n",
        "    This function provides a clean interface to interact with our language model,\n",
        "    allowing us to easily experiment with different prompts and settings.\n",
        "    \n",
        "    Args:\n",
        "        prompt (str): The user prompt/question you want to ask the model\n",
        "        temperature (float): Controls randomness and creativity\n",
        "            - 0.0-0.3: More deterministic, focused, consistent (good for factual tasks)\n",
        "            - 0.7-1.0: Balanced creativity (good for general tasks)\n",
        "            - 1.0+: More creative, diverse, unpredictable (good for creative writing)\n",
        "        system_prompt (str): Optional system prompt to set model behavior/persona\n",
        "            - Use this to control the model's role, tone, or expertise level\n",
        "    \n",
        "    Returns:\n",
        "        str: The model's response text\n",
        "    \"\"\"\n",
        "    # Prepare the message structure\n",
        "    messages = []\n",
        "    \n",
        "    # Add system prompt if provided (sets the model's behavior/role)\n",
        "    if system_prompt:\n",
        "        messages.append({\n",
        "            \"role\": \"system\",\n",
        "            \"content\": system_prompt\n",
        "        })\n",
        "    \n",
        "    # Add the user's prompt/question\n",
        "    messages.append({\n",
        "        \"role\": \"user\",\n",
        "        \"content\": prompt\n",
        "    })\n",
        "    \n",
        "    # Query the model via Ollama\n",
        "    response = ollama.chat(\n",
        "        model='llama3.2:3b',\n",
        "        messages=messages,\n",
        "        options={\n",
        "            'temperature': temperature\n",
        "        }\n",
        "    )\n",
        "    \n",
        "    return response['message']['content']\n",
        "\n",
        "print(\"‚úÖ Helper function 'query_model' created successfully!\")\n",
        "print(\"\\nüìñ Usage examples:\")\n",
        "print(\"   - Basic: query_model('What is AI?')\")\n",
        "print(\"   - With temperature: query_model('Write a story', temperature=1.0)\")\n",
        "print(\"   - With system prompt: query_model('Explain quantum physics', system_prompt='You are a teacher')\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 3. üå°Ô∏è Demo 1: Controlling Creativity with Temperature\n",
        "\n",
        "### What is Temperature?\n",
        "\n",
        "**Temperature** is one of the most important inference parameters. It controls the randomness and creativity of model outputs by adjusting how the model samples from its probability distribution.\n",
        "\n",
        "### Temperature Ranges\n",
        "\n",
        "| Temperature | Characteristics | Best For |\n",
        "|------------|----------------|----------|\n",
        "| **0.0 - 0.3** | Deterministic, focused, consistent | Factual Q&A, code generation, math problems |\n",
        "| **0.4 - 0.7** | Balanced, natural, slightly varied | General conversations, explanations |\n",
        "| **0.8 - 1.2** | Creative, diverse, unpredictable | Creative writing, brainstorming, storytelling |\n",
        "| **1.3+** | Highly random, experimental | Experimental use cases only |\n",
        "\n",
        "### üéØ Demo Task\n",
        "\n",
        "Let's see how temperature affects the same prompt. We'll ask the model to write a poem with different temperature settings and observe the differences in creativity and variation.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Demo 1: Temperature Effects - Low Temperature (Deterministic)\n",
        "prompt = \"Write a 4-line poem about a lonely robot on Mars.\"\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"üå°Ô∏è TEMPERATURE: 0.1 (Low - Deterministic & Focused)\")\n",
        "print(\"=\" * 80)\n",
        "print(f\"üìù Prompt: {prompt}\\n\")\n",
        "print(\"ü§ñ Model Response:\")\n",
        "print(\"-\" * 80)\n",
        "output_low = query_model(prompt, temperature=0.1)\n",
        "print(output_low)\n",
        "print(\"-\" * 80)\n",
        "print(\"\\nüí≠ Observation: Notice how the model is more focused and consistent.\")\n",
        "print(\"   Low temperature produces more predictable, factual outputs.\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\" * 80)\n",
        "print(\"üå°Ô∏è TEMPERATURE: 1.0 (High - Creative & Varied)\")\n",
        "print(\"=\" * 80)\n",
        "print(f\"üìù Prompt: {prompt}\\n\")\n",
        "print(\"ü§ñ Model Response:\")\n",
        "print(\"-\" * 80)\n",
        "output_high = query_model(prompt, temperature=1.0)\n",
        "print(output_high)\n",
        "print(\"-\" * 80)\n",
        "print(\"\\nüí≠ Observation: Notice how the model is more creative and varied.\")\n",
        "print(\"   High temperature produces more diverse, unexpected outputs.\\n\")\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"üìä KEY TAKEAWAY\")\n",
        "print(\"=\" * 80)\n",
        "print(\"üí° The SAME model with the SAME prompt produces DIFFERENT outputs!\")\n",
        "print(\"   Temperature is a powerful tool for controlling model behavior.\")\n",
        "print(\"   - Use LOW temperature for factual, consistent tasks\")\n",
        "print(\"   - Use HIGH temperature for creative, varied tasks\")\n",
        "print(\"=\" * 80)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 4. üß† Demo 2: Chain-of-Thought (CoT) Prompting\n",
        "\n",
        "### What is Chain-of-Thought?\n",
        "\n",
        "**Chain-of-Thought (CoT)** prompting is a technique that encourages the model to \"think step by step\" before providing a final answer. Instead of jumping directly to a conclusion, the model breaks down the problem into intermediate reasoning steps.\n",
        "\n",
        "### Why CoT Works\n",
        "\n",
        "1. **Mimics Human Reasoning**: Just like humans, models perform better when they work through problems systematically\n",
        "2. **Reduces Errors**: Breaking down complex problems helps catch mistakes early\n",
        "3. **Improves Accuracy**: Especially powerful for math, logic, and multi-step reasoning tasks\n",
        "4. **Makes Reasoning Transparent**: You can see how the model arrived at its answer\n",
        "\n",
        "### üéØ Demo Task\n",
        "\n",
        "Let's solve a multiplication problem (`123 √ó 76`) in two ways:\n",
        "1. **Baseline**: Direct question without step-by-step guidance\n",
        "2. **With CoT**: Explicitly asking for step-by-step reasoning\n",
        "\n",
        "**Expected Answer:** 9,348\n",
        "\n",
        "Watch how CoT prompting improves accuracy!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Demo 2: Chain-of-Thought - Baseline (No CoT)\n",
        "prompt_baseline = \"What is 123 * 76? Answer with just the number.\"\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"‚ùå BASELINE: Direct Question (No Chain-of-Thought)\")\n",
        "print(\"=\" * 80)\n",
        "print(f\"üìù Prompt: {prompt_baseline}\\n\")\n",
        "print(\"ü§ñ Model Response:\")\n",
        "print(\"-\" * 80)\n",
        "result_baseline = query_model(prompt_baseline, temperature=0.1)\n",
        "print(result_baseline)\n",
        "print(\"-\" * 80)\n",
        "print(f\"\\n‚úÖ Expected Answer: 9,348\")\n",
        "print(f\"üìä Model Answer: {result_baseline.strip()}\")\n",
        "print(\"\\nüí≠ Observation: Without step-by-step guidance, the model may struggle\")\n",
        "print(\"   with complex calculations or make errors.\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Demo 2: Chain-of-Thought - With Step-by-Step Reasoning\n",
        "prompt_cot = \"\"\"What is 123 * 76? Show me step by step. \n",
        "Multiply 100*76, then 20*76, then 3*76, and add them up.\"\"\"\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"‚úÖ CHAIN-OF-THOUGHT: Step-by-Step Reasoning\")\n",
        "print(\"=\" * 80)\n",
        "print(f\"üìù Prompt: {prompt_cot}\\n\")\n",
        "print(\"ü§ñ Model Response:\")\n",
        "print(\"-\" * 80)\n",
        "result_cot = query_model(prompt_cot, temperature=0.1)\n",
        "print(result_cot)\n",
        "print(\"-\" * 80)\n",
        "print(f\"\\n‚úÖ Expected Answer: 9,348\")\n",
        "print(\"\\nüí° KEY TAKEAWAY:\")\n",
        "print(\"   Notice how the step-by-step approach helps the model:\")\n",
        "print(\"   1. Break down the problem into manageable parts\")\n",
        "print(\"   2. Show its reasoning process (transparency!)\")\n",
        "print(\"   3. Achieve higher accuracy on complex tasks\")\n",
        "print(\"\\nüéØ Best Practice: Use CoT for any reasoning task!\")\n",
        "print(\"   - Math problems\")\n",
        "print(\"   - Logic puzzles\")\n",
        "print(\"   - Multi-step problem solving\")\n",
        "print(\"   - Any task requiring careful reasoning\")\n",
        "print(\"=\" * 80)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 5. üé≠ Interactive Demo: The \"Persona\" Challenge\n",
        "\n",
        "### Understanding System Prompts\n",
        "\n",
        "**System prompts** are special instructions that set the model's behavior, tone, and expertise level *before* the conversation begins. Think of them as setting the \"role\" or \"personality\" of the AI.\n",
        "\n",
        "### Why System Prompts Matter\n",
        "\n",
        "System prompts are incredibly powerful for:\n",
        "- **üéØ Audience Adaptation**: Explain the same concept to a 5-year-old vs. a PhD student\n",
        "- **üé≠ Role-Playing**: Transform the model into a specific character or expert\n",
        "- **üìù Style Control**: Set the tone (formal, casual, technical, friendly)\n",
        "- **üîß Behavior Shaping**: Define how the model should respond in different contexts\n",
        "\n",
        "### üéØ Your Challenge\n",
        "\n",
        "We'll ask the model to explain **Quantum Entanglement** using two completely different personas:\n",
        "1. **A 5-year-old teacher** - Simple, friendly, using everyday language\n",
        "2. **A Nobel Prize Physicist** - Technical, precise, using advanced terminology\n",
        "\n",
        "**Watch how the same model adapts its communication style!**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Interactive Demo: Persona Challenge - Part A\n",
        "# Task: Explain Quantum Entanglement as a 5-year-old teacher\n",
        "\n",
        "prompt = \"Explain Quantum Entanglement\"\n",
        "\n",
        "# Challenge A: 5-year-old teacher persona\n",
        "# Fill in the system prompt to make the model act like a friendly teacher for young children\n",
        "system_prompt_a = \"You are a specialized teacher for 5-year-old children. You explain complex topics using simple words, fun analogies, and everyday examples. You are patient, enthusiastic, and make learning fun!\"\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"üé≠ CHALLENGE A: 5-Year-Old Teacher Persona\")\n",
        "print(\"=\" * 80)\n",
        "print(f\"üìù System Prompt: {system_prompt_a}\")\n",
        "print(f\"‚ùì User Question: {prompt}\\n\")\n",
        "print(\"ü§ñ Model Response:\")\n",
        "print(\"-\" * 80)\n",
        "result_a = query_model(prompt, temperature=0.7, system_prompt=system_prompt_a)\n",
        "print(result_a)\n",
        "print(\"-\" * 80)\n",
        "print(\"\\nüí≠ Notice: Simple language, analogies, and child-friendly explanations\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Interactive Demo: Persona Challenge - Part B\n",
        "# Task: Explain Quantum Entanglement as a Nobel Prize Physicist\n",
        "\n",
        "prompt = \"Explain Quantum Entanglement\"\n",
        "\n",
        "# Challenge B: Nobel Prize Physicist persona\n",
        "# Fill in the system prompt to make the model act like an expert physicist\n",
        "system_prompt_b = \"You are a Nobel Prize-winning physicist with deep expertise in quantum mechanics. You use precise technical terminology, mathematical concepts, and advanced scientific language. You communicate with the precision and depth expected of a world-class researcher.\"\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"üé≠ CHALLENGE B: Nobel Prize Physicist Persona\")\n",
        "print(\"=\" * 80)\n",
        "print(f\"üìù System Prompt: {system_prompt_b}\")\n",
        "print(f\"‚ùì User Question: {prompt}\\n\")\n",
        "print(\"ü§ñ Model Response:\")\n",
        "print(\"-\" * 80)\n",
        "result_b = query_model(prompt, temperature=0.7, system_prompt=system_prompt_b)\n",
        "print(result_b)\n",
        "print(\"-\" * 80)\n",
        "print(\"\\nüí≠ Notice: Technical jargon, precise terminology, and advanced concepts\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### üìä Comparison & Analysis\n",
        "\n",
        "Now that you've seen both responses, let's analyze the differences:\n",
        "\n",
        "**Key Differences to Notice:**\n",
        "\n",
        "| Aspect | 5-Year-Old Teacher | Nobel Prize Physicist |\n",
        "|--------|-------------------|----------------------|\n",
        "| **Vocabulary** | Simple, everyday words | Technical, scientific terms |\n",
        "| **Sentence Structure** | Short, clear sentences | Complex, detailed explanations |\n",
        "| **Examples** | Relatable analogies (toys, games) | Mathematical and theoretical |\n",
        "| **Tone** | Friendly, enthusiastic | Formal, authoritative |\n",
        "| **Depth** | Surface-level concepts | Deep technical details |\n",
        "\n",
        "### üéØ Reflection Questions\n",
        "\n",
        "1. **Adaptation**: How well did the model adapt to each persona?\n",
        "2. **Consistency**: Did it maintain the persona throughout the response?\n",
        "3. **Effectiveness**: Which explanation would be better for different audiences?\n",
        "4. **Real-World Use**: Where could system prompts be useful in your projects?\n",
        "\n",
        "### üí° Key Insight\n",
        "\n",
        "**The same model, the same question, but completely different responses!**\n",
        "\n",
        "This demonstrates the power of system prompts. By simply changing the system prompt, you can:\n",
        "- Create educational tools for different age groups\n",
        "- Build specialized AI assistants (legal, medical, technical)\n",
        "- Control brand voice and communication style\n",
        "- Adapt content for different audiences automatically\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## üéì Key Takeaways & Best Practices\n",
        "\n",
        "### ‚úÖ What We Learned Today\n",
        "\n",
        "1. **üå°Ô∏è Temperature Control**\n",
        "   - Low temperature (0.1-0.3) ‚Üí Factual, consistent outputs\n",
        "   - High temperature (0.7-1.0) ‚Üí Creative, varied outputs\n",
        "   - **Best Practice**: Start with 0.7 for general use, adjust based on task\n",
        "\n",
        "2. **üß† Chain-of-Thought (CoT) Prompting**\n",
        "   - Break down complex problems into steps\n",
        "   - Dramatically improves accuracy on reasoning tasks\n",
        "   - **Best Practice**: Always use CoT for math, logic, and multi-step problems\n",
        "\n",
        "3. **üé≠ System Prompts**\n",
        "   - Control model behavior, tone, and expertise\n",
        "   - Adapt content for different audiences\n",
        "   - **Best Practice**: Define the role clearly and be specific about desired behavior\n",
        "\n",
        "### üöÄ Prompt Engineering Best Practices\n",
        "\n",
        "| Technique | When to Use | Example |\n",
        "|-----------|-------------|---------|\n",
        "| **Low Temperature** | Factual Q&A, code, math | `temperature=0.1` |\n",
        "| **High Temperature** | Creative writing, brainstorming | `temperature=1.0` |\n",
        "| **Chain-of-Thought** | Reasoning, calculations, logic | \"Show me step by step...\" |\n",
        "| **System Prompts** | Role-playing, audience adaptation | \"You are a [role]...\" |\n",
        "\n",
        "### üíº Real-World Applications\n",
        "\n",
        "- **Customer Support**: System prompt sets helpful, professional tone\n",
        "- **Educational Tools**: Adapt explanations for different grade levels\n",
        "- **Content Creation**: Control writing style and voice\n",
        "- **Code Generation**: Use low temperature + CoT for accurate code\n",
        "- **Data Analysis**: CoT helps models reason through complex problems\n",
        "\n",
        "### üéØ Next Steps for Practice\n",
        "\n",
        "1. **Experiment**: Try different temperature values on the same prompt\n",
        "2. **Practice CoT**: Apply step-by-step reasoning to other problems\n",
        "3. **Create Personas**: Design system prompts for specific use cases\n",
        "4. **Combine Techniques**: Use CoT + system prompts together\n",
        "5. **Iterate**: Prompt engineering is iterative - refine and improve!\n",
        "\n",
        "---\n",
        "\n",
        "## üìö Additional Resources\n",
        "\n",
        "- **Temperature Guide**: Experiment with values between 0.0 and 2.0\n",
        "- **CoT Variations**: Try \"think step by step\", \"show your work\", \"reason through this\"\n",
        "- **System Prompt Templates**: Create a library of effective personas\n",
        "- **Multi-turn Conversations**: Build on previous responses for complex tasks\n",
        "\n",
        "---\n",
        "\n",
        "## üéâ Congratulations!\n",
        "\n",
        "You've mastered the fundamentals of prompt engineering! These techniques will help you get the best results from any language model. Remember: **the quality of your prompts directly determines the quality of your outputs.**\n",
        "\n",
        "**Happy Prompting! üöÄ**\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
